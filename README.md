# NLP_datasets_collection
I will collect the datasets for NLP. The collection will keep updating.   
我将不断收集我遇到的各种NLP数据集以备不时之需。本集合将不断地进行更新。  

排名不分先后，仅按我添加的先后顺序。若有侵权，请电邮我告知删除。  


# Chinese Courpus:
1. > ### 大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP
   > [Github](https://github.com/brightmart/nlp_chinese_corpus)

2. > ### 搜狗实验室语料集合
   > [语料数据](http://www.sogou.com/labs/resource/list_yuliao.php)

# Pretrained chinese Word Vectors(embeddings):
1. > ### 100+ Chinese Word Vectors 上百种预训练中文词向量
   > [Github](https://github.com/Embedding/Chinese-Word-Vectors)
   
2. > ### Tencent AI Lab Embedding Corpus for Chinese Words and Phrases
   > [URL](https://ai.tencent.com/ailab/nlp/embedding.html)

# English Corpus:
1. > ### conversational-datasets：对话AI大规模数据集 
   > ### conversational-datasets: A collection of large datasets for conversational response selection.
   > [Github](https://github.com/PolyAI-LDN/conversational-datasets)

2. > ### LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记小说语料  
   > ### LitBank: Annotated dataset of 100 works of fiction to support tasks in natural language processing and the computational humanities  
   > [Github](https://github.com/dbamman/litbank) [Paper](http://people.ischool.berkeley.edu/~dbamman/pubs/pdf/naacl2019_literary_entities.pdf)
   
3. > ### Fake News Corpus: A dataset of millions of news articles scraped from a curated list of data sources 
   > [Github](https://github.com/several27/FakeNewsCorpus)
